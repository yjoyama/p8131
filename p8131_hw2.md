Homework 2
================
Yuki Joyama
2024-02-16

## 1

``` r
# load data
dose = c(0, 1, 2, 3, 4)
num = c(30, 30, 30, 30, 30)
dead = c(2, 8, 15, 23, 27)
data_1 = data.frame(dose, num, dead)

# visualization 
# plot(data_1$dose, data_1$dead/data_1$num, xlab = 'dose', ylab = 'Proportion dying', cex = 1.5, pch = 19, cex.lab = 1.6, cex.axis = 1.5)

# data prep 
x = data_1$dose
y = data_1$dead
m = data_1$num
resp = cbind(y, m-y)
```

Now, I will fit the model $g(P(dying)) = \alpha + \beta X$ with logit,
probit, and complementary log-log links.

``` r
# fit logistic model, logit 
glm_logit = glm(resp ~ x, family = binomial(link = 'logit'))
summary(glm_logit)
```

    ## 
    ## Call:
    ## glm(formula = resp ~ x, family = binomial(link = "logit"))
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept)  -2.3238     0.4179  -5.561 2.69e-08 ***
    ## x             1.1619     0.1814   6.405 1.51e-10 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 64.76327  on 4  degrees of freedom
    ## Residual deviance:  0.37875  on 3  degrees of freedom
    ## AIC: 20.854
    ## 
    ## Number of Fisher Scoring iterations: 4

``` r
# fit logistic model, probit 
glm_probit = glm(resp ~ x, family = binomial(link = 'probit'))
summary(glm_probit)
```

    ## 
    ## Call:
    ## glm(formula = resp ~ x, family = binomial(link = "probit"))
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept) -1.37709    0.22781  -6.045 1.49e-09 ***
    ## x            0.68638    0.09677   7.093 1.31e-12 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 64.76327  on 4  degrees of freedom
    ## Residual deviance:  0.31367  on 3  degrees of freedom
    ## AIC: 20.789
    ## 
    ## Number of Fisher Scoring iterations: 4

``` r
# fit logistic model, cloglog 
glm_cloglog = glm(resp ~ x, family = binomial(link = 'cloglog'))
summary(glm_cloglog)
```

    ## 
    ## Call:
    ## glm(formula = resp ~ x, family = binomial(link = "cloglog"))
    ## 
    ## Coefficients:
    ##             Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept)  -1.9942     0.3126  -6.378 1.79e-10 ***
    ## x             0.7468     0.1094   6.824 8.86e-12 ***
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 64.7633  on 4  degrees of freedom
    ## Residual deviance:  2.2305  on 3  degrees of freedom
    ## AIC: 22.706
    ## 
    ## Number of Fisher Scoring iterations: 5

### a

``` r
# 95% CI for beta, logit
beta = glm_logit$coefficients[2]
se = sqrt(vcov(glm_logit)[2, 2])
round(beta + c(qnorm(0.025), -qnorm(0.025)) * se, 3)
```

    ## [1] 0.806 1.517

``` r
# 95% CI for beta, probit
beta = glm_probit$coefficients[2]
se = sqrt(vcov(glm_probit)[2, 2])
round(beta + c(qnorm(0.025), -qnorm(0.025)) * se, 3)
```

    ## [1] 0.497 0.876

``` r
# 95% CI for beta, cloglog
beta = glm_cloglog$coefficients[2]
se = sqrt(vcov(glm_cloglog)[2, 2])
round(beta + c(qnorm(0.025), -qnorm(0.025)) * se, 3)
```

    ## [1] 0.532 0.961

``` r
# p_hat(dying|x = 0.01), logit
predict(glm_logit, newdata = data.frame(x = 0.01), type = 'response')
```

    ##          1 
    ## 0.09011997

``` r
# p_hat(dying|x = 0.01), probit
predict(glm_probit, newdata = data.frame(x = 0.01), type = 'response')
```

    ##         1 
    ## 0.0853078

``` r
# p_hat(dying|x = 0.01), cloglog
predict(glm_cloglog, newdata = data.frame(x = 0.01), type = 'response')
```

    ##         1 
    ## 0.1281601

| Model     | Estimate of beta |   CI for beta | Deviance | p_hat(dying\|x=0.01) |
|:----------|-----------------:|--------------:|---------:|---------------------:|
| logit     |            1.162 | (0.806-1.517) |    0.379 |               0.0901 |
| probit    |            0.686 | (0.497-0.876) |    0.314 |               0.0853 |
| c-log-log |            0.747 | (0.532-0.961) |    2.231 |                0.128 |

### b

## 2

``` r
# load data
amount = seq(10, 90, 5)
offers = c(4, 6, 10, 12, 39, 36, 22, 14, 10, 12, 8, 9, 3, 1, 5, 2, 1)
enrolls = c(0, 2, 4, 2, 12, 14, 10, 7, 5, 5, 3, 5, 2, 0, 4, 2, 1)

data_2 = data.frame(amount, offers, enrolls)
```

### a
